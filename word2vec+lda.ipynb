{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.rsm-msba/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from gensim import models,corpora,similarities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import TaggedDocument,Doc2Vec\n",
    "from gensim.models import LdaModel\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from gensim.models import word2vec\n",
    "import sklearn.metrics as metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropbox_folder = None\n",
    "\n",
    "for dirname, dirnames, filenames in os.walk(os.path.expanduser('~')):\n",
    "    for subdirname in dirnames:\n",
    "        if(subdirname == 'Dropbox'):\n",
    "            dropbox_folder = os.path.join(dirname, subdirname)\n",
    "            break\n",
    "    if dropbox_folder:\n",
    "        break\n",
    "\n",
    "with open(dropbox_folder + 'data.txt', 'r') as file:\n",
    "    doc = [c.split(',') for c in file.read().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train word2vec model\n"
     ]
    }
   ],
   "source": [
    "# train word2vec model\n",
    "size_word=100\n",
    "size_lda=12\n",
    "print(\"train word2vec model\")\n",
    "model_wv=word2vec.Word2Vec(doc, size=size_word,workers=2,min_count=1,iter=10)\n",
    "dictionary = corpora.Dictionary(doc)\n",
    "corpus = [ dictionary.doc2bow(text) for text in doc ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train lda model\n"
     ]
    }
   ],
   "source": [
    "#train lda model\n",
    "print(\"train lda model\")\n",
    "lda = LdaModel(corpus=corpus,id2word=dictionary, num_topics=size_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the topic distribution of the new document [(4, 0.7591665), (7, 0.22309174)]\n",
      "Proportion of the topic 0.034*\"number\" + 0.032*\"card\" + 0.015*\"email\" + 0.015*\"paper\" + 0.015*\"information\" + 0.012*\"address\" + 0.010*\"date\" + 0.010*\"account\" + 0.009*\"credit\" + 0.007*\"read\"\t0.759166\n",
      "\n",
      "Proportion of the topic 0.030*\"glasses\" + 0.009*\"horizon\" + 0.009*\"connected\" + 0.008*\"send\" + 0.008*\"iphone\" + 0.008*\"connect\" + 0.007*\"power\" + 0.007*\"verizon\" + 0.007*\"button\" + 0.007*\"device\"\t0.223092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lda.get_document_topics\n",
    "for doc_num in doc:\n",
    "    doc_bow=dictionary.doc2bow(doc_num)\n",
    "    doc_lda = lda[doc_bow]\n",
    "    print (\"the topic distribution of the new document\",doc_lda)\n",
    "    break\n",
    "for topic in doc_lda:\n",
    "    print (\"Proportion of the topic\",\"%s\\t%f\\n\"%(lda.print_topic(topic[0]), topic[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(82, 0.014549933), (705, 0.014494144), (49, 0.012492223), (164, 0.0117205605), (0, 0.008091554), (80, 0.007492071), (325, 0.0074825534), (90, 0.006868124), (950, 0.0066822777), (39, 0.0066119293)]\n"
     ]
    }
   ],
   "source": [
    "print (lda.get_topic_terms(0, topn=10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO.0 topic NO.82 main word password\n",
      "NO.0 topic NO.705 main word teamviewer\n",
      "NO.0 topic NO.49 main word computer\n",
      "NO.0 topic NO.164 main word email\n",
      "NO.0 topic NO.0 main word address\n",
      "NO.0 topic NO.80 main word page\n",
      "NO.0 topic NO.325 main word minutes\n",
      "NO.0 topic NO.90 main word read\n",
      "NO.0 topic NO.950 main word google\n",
      "NO.0 topic NO.39 main word account\n",
      "[]\n",
      "[(4, 0.7592648), (7, 0.22299334)]\n"
     ]
    }
   ],
   "source": [
    "# get the main word in each topic\n",
    "aa=[x[0] for x in lda.get_topic_terms(0, topn=10)]\n",
    "for it in aa:\n",
    "    print (\"NO.%d topic NO.%d main word\"%(0,it), dictionary[it])\n",
    "\n",
    "print  (lda.get_term_topics(0, minimum_probability=0.05))\n",
    "\n",
    "print (lda.get_document_topics(corpus[0], minimum_probability=None, minimum_phi_value=0.1, per_word_topics=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping the topic to the word2vec space\n"
     ]
    }
   ],
   "source": [
    "print (\"Mapping the topic to the word2vec space\")\n",
    "the_id=[]\n",
    "the_vl=[]\n",
    "the_w =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate the weight of each word in the topic\n"
     ]
    }
   ],
   "source": [
    "print (\"Calculate the weight of each word in the topic\")\n",
    "for x in range(size_lda):\n",
    "    the_id.append([xx[0] for xx in lda.get_topic_terms(x,topn=5)])\n",
    "    the_sum=sum([xx[1] for xx in lda.get_topic_terms(x,topn=5)])\n",
    "    the_w.append([xx[1]/the_sum for xx in lda.get_topic_terms(x,topn=5)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mapping to coordinates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Mapping the topic to the word2vec space\n",
    "print (\" mapping to coordinates\")\n",
    "m=0\n",
    "the_wv=np.zeros([size_lda,size_word])\n",
    "\n",
    "for it in the_id:\n",
    "    n=0\n",
    "    for it_id in it:\n",
    "        word_t=dictionary[it_id]\n",
    "        #print (word_t+\"**\",np.shape(model_wv[word_t]),the_w[m][n])\n",
    "        the_wv[m]+=[x_word*the_w[m][n] for x_word in model_wv[word_t]]\n",
    "        n+=1\n",
    "    m+=1\n",
    "doc_word=np.zeros([len(doc),size_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping the document to word2vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Mapping the documents to the word2vec space\n",
    "print (\"Mapping the document to word2vec\")\n",
    "m=0\n",
    "for each_doc in doc:\n",
    "    for each_word in each_doc:\n",
    "        #print each_word\n",
    "        doc_word[m]+=model_wv[each_word]\n",
    "    n=0\n",
    "    for doc_word_each in doc_word[m]:\n",
    "        doc_word[m][n]=doc_word[m][n]/len(doc_word[m])\n",
    "        n+=1\n",
    "    m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate the distance bewteen the document to topic\n"
     ]
    }
   ],
   "source": [
    "# calculate the distance bewteen the each document to each topic\n",
    "print (\"calculate the distance bewteen the document to topic\")\n",
    "def destince(a,b):\n",
    "    dt=0\n",
    "    for each_dt in range(len(a)):\n",
    "        dt+=(a[each_dt]-b[each_dt])*(a[each_dt]-b[each_dt])\n",
    "    return np.sqrt(dt)\n",
    "doc_t=np.zeros([len(doc_word),size_lda])\n",
    "m=0\n",
    "for each_d in doc_word:\n",
    "    n=0\n",
    "    for each_t in the_wv:\n",
    "        doc_t[m][n]=destince(each_d,each_t)\n",
    "        n+=1\n",
    "    m+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the nearest topic the domain topic\n",
    "topic=[]\n",
    "for i in doc_t:\n",
    "    topic.append(np.argmin(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 4922, 1: 1719, 10: 481, 8: 33, 3: 25, 5: 25, 9: 25, 2: 21, 11: 8, 7: 7, 4: 1})\n"
     ]
    }
   ],
   "source": [
    "##calculate the distribution of the topic\n",
    "import collections\n",
    "counter=collections.Counter(topic)\n",
    "print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
